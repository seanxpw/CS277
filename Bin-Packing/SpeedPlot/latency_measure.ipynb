{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78xfGN4FJH0X",
        "outputId": "e5c6bd6a-b34a-4d35-899e-cfdccfb27a07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.utils.benchmark as benchmark\n",
        "from torch import nn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Bin Packing: Approximate (First Fit)\n",
        "# -------------------------------\n",
        "def first_fit(items, bin_capacity):\n",
        "    bins_content = []\n",
        "    bin_remaining_capacity = []\n",
        "    valid_items = [item for item in items if 0 < item <= bin_capacity]\n",
        "\n",
        "    for item_size in valid_items:\n",
        "        placed = False\n",
        "        for i in range(len(bins_content)):\n",
        "            if item_size <= bin_remaining_capacity[i]:\n",
        "                bins_content[i].append(item_size)\n",
        "                bin_remaining_capacity[i] -= item_size\n",
        "                placed = True\n",
        "                break\n",
        "        if not placed:\n",
        "            bins_content.append([item_size])\n",
        "            bin_remaining_capacity.append(bin_capacity - item_size)\n",
        "    return len(bins_content)"
      ],
      "metadata": {
        "id": "bo5OKQ75JZQB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Bin Packing: Optimal (Backtracking)\n",
        "# -------------------------------\n",
        "min_bins_solution_global = float('inf')\n",
        "\n",
        "def solve_optimal_recursive(items_to_pack, bin_capacity, bins_count, rem_cap):\n",
        "    global min_bins_solution_global\n",
        "\n",
        "    if not items_to_pack:\n",
        "        min_bins_solution_global = min(min_bins_solution_global, bins_count)\n",
        "        return\n",
        "\n",
        "    if bins_count >= min_bins_solution_global:\n",
        "        return\n",
        "\n",
        "    item = items_to_pack[0]\n",
        "    rest = items_to_pack[1:]\n",
        "\n",
        "    for i in range(bins_count):\n",
        "        if item <= rem_cap[i]:\n",
        "            rem_cap[i] -= item\n",
        "            solve_optimal_recursive(rest, bin_capacity, bins_count, rem_cap)\n",
        "            rem_cap[i] += item\n",
        "\n",
        "    rem_cap.append(bin_capacity - item)\n",
        "    solve_optimal_recursive(rest, bin_capacity, bins_count + 1, rem_cap)\n",
        "    rem_cap.pop()\n",
        "\n",
        "def optimal_bin_packing(items, bin_capacity):\n",
        "    global min_bins_solution_global\n",
        "    min_bins_solution_global = float('inf')\n",
        "    valid_items = sorted([item for item in items if 0 < item <= bin_capacity], reverse=True)\n",
        "    if not valid_items: return 0\n",
        "    solve_optimal_recursive(valid_items, bin_capacity, 0, [])\n",
        "    return min_bins_solution_global\n",
        "\n"
      ],
      "metadata": {
        "id": "B2x1PlQEJjC6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# PyTorch Modules\n",
        "# -------------------------------\n",
        "class FirstFitModule(nn.Module):\n",
        "    def __init__(self, bin_capacity):\n",
        "        super().__init__()\n",
        "        self.bin_capacity = bin_capacity\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor):\n",
        "        return torch.tensor([first_fit(x.tolist(), self.bin_capacity) for x in inputs])\n",
        "\n",
        "class OptimalModule(nn.Module):\n",
        "    def __init__(self, bin_capacity):\n",
        "        super().__init__()\n",
        "        self.bin_capacity = bin_capacity\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor):\n",
        "        return torch.tensor([optimal_bin_packing(x.tolist(), self.bin_capacity) for x in inputs])\n"
      ],
      "metadata": {
        "id": "e5vWFRu9JnyD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Benchmarking Function\n",
        "# -------------------------------\n",
        "def benchmark_latency(model, input_tensor):\n",
        "    print(f\"\\nðŸ§ª Benchmarking: {model.__class__.__name__}\")\n",
        "    model.eval()\n",
        "\n",
        "    # Warm-up\n",
        "    model(input_tensor[:1])\n",
        "\n",
        "    # Batch size = 2000\n",
        "    t_batch = benchmark.Timer(\n",
        "        stmt=\"model(inputs)\",\n",
        "        globals={\"model\": model, \"inputs\": input_tensor}\n",
        "    )\n",
        "    print(t_batch.timeit(5))\n",
        "\n",
        "    # Batch size = 1\n",
        "    t_single = benchmark.Timer(\n",
        "        stmt=\"model(inputs[:1])\",\n",
        "        globals={\"model\": model, \"inputs\": input_tensor}\n",
        "    )\n",
        "    print(t_single.timeit(5))\n"
      ],
      "metadata": {
        "id": "mXiWJXpYKDfC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Generate Random Data\n",
        "# -------------------------------\n",
        "BIN_CAPACITY = 100\n",
        "NUM_SAMPLES = 2000\n",
        "ITEMS_PER_SAMPLE = 30\n",
        "\n",
        "np.random.seed(42)\n",
        "items_np = np.random.randint(1, BIN_CAPACITY + 1, size=(NUM_SAMPLES, ITEMS_PER_SAMPLE))\n",
        "input_tensor = torch.tensor(items_np, dtype=torch.int16)\n",
        "\n",
        "# -------------------------------\n",
        "# Run Benchmarks\n",
        "# -------------------------------\n",
        "ff_model = FirstFitModule(bin_capacity=BIN_CAPACITY)\n",
        "opt_model = OptimalModule(bin_capacity=BIN_CAPACITY)\n",
        "\n",
        "benchmark_latency(ff_model, input_tensor)\n",
        "benchmark_latency(opt_model, input_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozcCGTZzKOnV",
        "outputId": "47d9d94a-c969-4e3c-b2b0-3d83ab2f0689"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ§ª Benchmarking: FirstFitModule\n",
            "<torch.utils.benchmark.utils.common.Measurement object at 0x7edd4ab17410>\n",
            "model(inputs)\n",
            "  40.41 ms\n",
            "  1 measurement, 5 runs , 1 thread\n",
            "<torch.utils.benchmark.utils.common.Measurement object at 0x7edd4c5eae90>\n",
            "model(inputs[:1])\n",
            "  39.83 us\n",
            "  1 measurement, 5 runs , 1 thread\n",
            "\n",
            "ðŸ§ª Benchmarking: OptimalModule\n",
            "<torch.utils.benchmark.utils.common.Measurement object at 0x7ede0c536210>\n",
            "model(inputs)\n",
            "  165.99 s\n",
            "  1 measurement, 5 runs , 1 thread\n",
            "<torch.utils.benchmark.utils.common.Measurement object at 0x7edd4c5bff90>\n",
            "model(inputs[:1])\n",
            "  140.76 us\n",
            "  1 measurement, 5 runs , 1 thread\n"
          ]
        }
      ]
    }
  ]
}